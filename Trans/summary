可以做这样的总结：
传统的word2vec和transformer结构在embedding层的异同：
不同点：
1、word2vec是静态的，训练完成之后不会优化，属于无监督学习，而embedding层和后续模型一起训练，是动态的，并且通过transformer结构的自注意力机制，可以实现一词多义，而word2vec仅能实现一词一义
2、传统的word2vec，是先训练词嵌入矩阵，再训练模型，两者是分开操作的；而transformer模型是将embedding层和模型放在一起训练的
3、传统的word2vec是先将输入单词进行one-hot编码，然后再通过降维的方式将每个单词变成可训练的向量，而transformer结构中的embedding层是通过查表的方式获取对应的可训练向量
相同点
1、在经过训练之后，词嵌入矩阵或者tranformer结构中embedding层训练的表，都可以表示词语之间的相关性
2、词嵌入矩阵的参数和transformer结构中embedding层表的参数是一致的，都是最大单词数*降维之后的维度数


关于自注意力机制：Q可以理解为当前词，K可以理解为上下文词，V可以理解为输入，Q和K的点乘用于计算当前词和上下文词之间的相关性


对于自注意力机制，我有这样子的理解，有这样两句话，第一句话是：我喜欢炒股，因为炒股可以赚钱；第二句话是，炒股是一件有风险的事情；经过Q和K的计算之后，对于第一句话，炒股和赚钱的关联性更强，应该在结果矩阵中相关性更好，而在第二句话中，炒股和风险关联性更强，在结果矩阵中相关性更好；最终再和V进行计算，第一句话的计算结果，炒股和赚钱，被点亮更多；而在第二句话中，炒股和风险被点亮更多

自注意力机制和跨注意力机制：
Q、K、V均来源于输入，在自注意力机制中，GPT模型
Q和K有不同的来源，在跨注意力机制中